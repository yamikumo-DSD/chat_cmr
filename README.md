<h2>LLM inference GUI for Jupyter notebook</h2>
<br>
This is an LLM-powered chat interface integrated with voice synthesis model, web search-based RAG, and python environment.<br>
For better results, I strongly recommend you to select a model large enough or trained for tool use.<br>
<br>
<h3>Features</h3>
1. Streaming output.<br>
2. Context shifting manipulating KV-Cache significantly reducing evaluation time (called StreamingLLM).<br>
3. Web search tool with vector search-based RAG with <b>local embedding</b>.<br>
4. Automatic execution of Python code generated by LLM.<br>
5. Integration to Japanese text-to-speech model called style-bert-vits2.<br>
6. Easy to integrate any tools.<br>
7. Configurable UIs.<br>
<br>
<h2>Prerequisite</h2>
1. Jupyter must be installed.<br>
<code>$ pip install jupyterlab</code><br>
2. Activate ipywidgets by one of following commands:<br>
<pre><code># jupyter-lab
$ jupyter labextension install @jupyter-widgets/jupyterlab-manager
# jupyter notebook / Google Colab<br>
$ jupyter nbextension enable --py widgetsnbextension</code></pre>
3. Install dependencies.<br>
<code>$ pip install -r requirements.txt</code><br>
<h2>Screen Shots</h2>
<h3>Python execution</h3>
<img src=https://github.com/yamikumo-DSD/chat_cmr/blob/main/SS1.png>
<h3>Web search</h3>
<img src=https://github.com/yamikumo-DSD/chat_cmr/blob/main/SS2.png>
