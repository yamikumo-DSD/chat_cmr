{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8b1b04-e2ed-4184-be52-0584f617ab9d",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "1. Jupyter must be installed.<br>\n",
    "<code>\\$ pip install jupyterlab</code><br>\n",
    "2. Activate ipywidgets by one of following commands:<br>\n",
    "    2-1. jupyter-lab<br>\n",
    "<code>\\$ jupyter labextension install @jupyter-widgets/jupyterlab-manager</code><br>\n",
    "    2-2. jupyter notebook / Google Colab<br>\n",
    "<code>\\$ jupyter nbextension enable --py widgetsnbextension</code>\n",
    "3. Install dependencies.<br>\n",
    "<code>\\$ pip install -r requirements.txt</code><br>\n",
    "4. *(Optional)* You might want to unlock websocket message size limit to send large files or to play longer sounds (the limit is 10MB by default).<br>\n",
    "<code>\\$ jupyter notebook --generate-config</code><br>\n",
    "After running the command, edit following line of the config file:<br>\n",
    "<code>c.ServerApp.tornado_settings = {\"websocket_max_message_size\":100\\*1024\\*1024} # Your preference</code><br>\n",
    "<br>\n",
    "<a href=https://github.com/yamikumo-DSD/chat_cmr/tree/main>GitHub link for the repository</a>\n",
    "## ⇩⇩ RUN ALL CELLS BELOW ⇩⇩\n",
    "Run cells all the way down to the bottom!<br>\n",
    "<font color=#FFFF00>**CAUTION!**</font> First run will take time to download a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed8c855-4eab-4fd6-8d95-2149dbff4a81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "ignore_warnings = lambda : simplefilter('ignore')\n",
    "ignore_warnings()\n",
    "\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import dataclasses\n",
    "from style_bert_vits2.tts_model import TTSModel\n",
    "\n",
    "from IPython.display import clear_output, HTML\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import lib.markdown as md\n",
    "from lib.infrastructure import ForgetableContext, LlamaCpp\n",
    "from lib.uis import wait_for_change, Switchable\n",
    "from lib.utils import now, mixed2katakana, replace_text\n",
    "\n",
    "import session_states as session\n",
    "import agent_tools as tools\n",
    "from global_settings import *\n",
    "from prompt_builders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa76e7b-03c2-40a7-92d7-16cb69c5d8e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_default_dirs() -> None:\n",
    "    default_dirs = [GGUF_DIR, AGENT_WORKING_DIR, LOG_DIR, TTS_ASSET_ROOT, CACHE_DIR]\n",
    "    for directory in default_dirs:\n",
    "        if os.path.isdir(directory):\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"Directory \\\"{directory}\\\" does not exsist. Creating.\")\n",
    "            os.makedirs(directory)\n",
    "        except FileExistsError as e:\n",
    "            print(e)\n",
    "            \n",
    "make_default_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fb6bf1-2861-44ba-a933-c6114d7e3529",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reload_instruction(instruction_file_path: str):\n",
    "    instructions = \"\"\n",
    "    with open(instruction_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        instructions = ''.join(lines)\n",
    "        \n",
    "    return instructions\n",
    "\n",
    "prompt_builder = Llama3PromptBuilder(\n",
    "    reload_instruction(\"system_prompt_template/l3_sys_ppt_gen_16.txt\"), \n",
    "    session.context\n",
    ")\n",
    "\n",
    "def change_prompt_builder(args):\n",
    "    global prompt_builder\n",
    "    context = session.context\n",
    "    prompt_builder = {\n",
    "        \"Command R\": CommandRPromptBuilder(reload_instruction(\"./system_prompt_template/cmdr_sys_ppt_gen_16.txt\"), context),\n",
    "        \"Llama-3 Instruct\": Llama3PromptBuilder(reload_instruction(\"./system_prompt_template/l3_sys_ppt_gen_16.txt\"), context),\n",
    "        \"ChatML\": ChatMLPromptBuilder(reload_instruction(\"./system_prompt_template/chatml_sys_ppt_gen_16.txt\"), context),\n",
    "        \"Llama-2 Instruct\": Llama2PromptBuilder(reload_instruction(\"./system_prompt_template/l2_sys_ppt_gen_16.txt\"), context),\n",
    "        \"Llama-2 Instruct JA\": JaCommMSPromptBuilder(reload_instruction(\"./system_prompt_template/ja_community_ms_sys_ppt_gen_16.txt\"), context),\n",
    "        \"Gemma 2 Instruct\": Gemma2Instruct(reload_instruction(\"./system_prompt_template/gemma_2_it_sys_ppt_gen_16.txt\"), context),\n",
    "    }[session.template_selector.value]\n",
    "\n",
    "session.template_selector.observe(change_prompt_builder, \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed7b25fa-acdf-4ba4-9522-ea8979e90ff7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Updates GUIs.\n",
    "def get_bigger(args):        \n",
    "    nls = session.field.value.count('\\n')\n",
    "    session.field.rows = nls + 1 if nls >= 1 else 2\n",
    "session.field.observe(get_bigger, 'value')\n",
    "\n",
    "\n",
    "def fix_max_gen_tokens(args) -> None:\n",
    "    options = session.llama_cpp_options\n",
    "    options.define_max_gen_tokens.value = min(\n",
    "        options.define_max_gen_tokens.value, \n",
    "        session.n_ctx//2\n",
    "    )\n",
    "    \n",
    "session.llama_cpp_options.define_max_gen_tokens.observe(fix_max_gen_tokens, \"value\")\n",
    "\n",
    "def set_guessing_image(show: bool) -> None:\n",
    "    if not session.guessing_image:\n",
    "        with open('images/guessing.gif' if show else 'images/empty.png', 'rb') as f:\n",
    "            session.guessing_image = widgets.Image(value=f.read(), width=50, height=50)\n",
    "        \n",
    "    with open('images/guessing.gif' if show else 'images/empty.png', 'rb') as f:\n",
    "        session.guessing_image.value = f.read() \n",
    "        session.guessing_image.width = 50\n",
    "        session.guessing_image.height = 50\n",
    "\n",
    "set_guessing_image(False)\n",
    "\n",
    "\n",
    "def set_buttons(disabled: bool) -> None:\n",
    "    for b in session.buttons:\n",
    "        b.disabled = disabled\n",
    "\n",
    "def disable_uis(func):\n",
    "    from functools import wraps\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        set_buttons(disabled=True)\n",
    "        set_guessing_image(True)\n",
    "        result = func(*args, **kwargs)\n",
    "        set_guessing_image(False)\n",
    "        set_buttons(disabled=False)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1209f7c2-c82e-428d-b2c7-9d3af188195f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def refresh_gguf_list(sender=None):\n",
    "    selector = session.llama_cpp_options.gguf_selector\n",
    "    session.ggufs = [item for item in os.listdir(GGUF_DIR) if item.endswith(\".gguf\")]\n",
    "    ggufs = session.ggufs\n",
    "    \n",
    "    selector.options = ggufs if len(ggufs) > 0 else [\"No gguf in dir\"]\n",
    "    selector.value = ggufs[0] if len(ggufs) > 0 else \"No gguf in dir\"\n",
    "\n",
    "    \n",
    "def _load_model(gguf_path: str) -> None:\n",
    "    options = session.llama_cpp_options\n",
    "    assert session.n_ctx > session.max_gen_tokens, ValueError(\"max_gen_tokens must be smaller than n_ctx\")\n",
    "\n",
    "    try:\n",
    "        with session.debug:\n",
    "            session.model = LlamaCpp(\n",
    "                model_path=gguf_path,\n",
    "                n_gpu_layers=options.n_gpu_layers.value,\n",
    "                n_batch=1024,\n",
    "                n_ctx=options.define_n_ctx.value,\n",
    "                use_mlock=True,\n",
    "                flash_attn=options.flash_attention.value,\n",
    "                verbose=True,\n",
    "                embedding=False,\n",
    "                type_k=8 if options.quantize_kv.value else None, # Default is FP16\n",
    "                type_v=8 if options.quantize_kv.value else None, # Default is FP16\n",
    "            )\n",
    "        session.n_ctx = options.define_n_ctx.value\n",
    "        session.max_gen_tokens = options.define_max_gen_tokens.value\n",
    "        session.active_gguf = os.path.basename(gguf_path)\n",
    "        session.set_gguf_viewer(session.active_gguf)\n",
    "    except ValueError as e:\n",
    "        with session.debug:\n",
    "            print(e)\n",
    "    \n",
    "def _unload_model() -> None:\n",
    "    import gc\n",
    "    import llama_cpp\n",
    "    from packaging.version import Version\n",
    "\n",
    "    if session.model == None:\n",
    "        return\n",
    "\n",
    "    # Explicitly free the model. This is very recently implemented method.\n",
    "    # https://github.com/abetlen/llama-cpp-python/pull/1513\n",
    "    if Version(llama_cpp.__version__) >= Version(\"0.2.78\"):\n",
    "        session.model.llama.close()\n",
    "    \n",
    "    session.model = None\n",
    "    session.active_gguf = \"\"\n",
    "    session.unset_gguf_viewer()\n",
    "    gc.collect()\n",
    "    \n",
    "@disable_uis\n",
    "def reload_model(sender=None) -> None:\n",
    "    _unload_model()\n",
    "    _load_model(os.path.join(GGUF_DIR, session.llama_cpp_options.gguf_selector.value))\n",
    "    initialize()\n",
    "    \n",
    "@disable_uis\n",
    "def unload_model(sender=None) -> None:\n",
    "    _unload_model()\n",
    "    initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d3d8b-a24e-4c23-83c5-50de61ef244f",
   "metadata": {},
   "source": [
    "## Download default model\n",
    "### Remove one cell below to disable default model download at first run.\n",
    "This process will be automatically skipped if a directory <code>./ggufs</code> is not empty.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d203472-ff36-4072-99c5-e97aa41cec0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def download_default_model(url: str) -> None:\n",
    "    if len(session.ggufs) > 0: return\n",
    "        \n",
    "    print(f\"Directory \\\"{GGUF_DIR}\\\" is empty. Downloading default model.\")\n",
    "    MAX_CONTINUE = 10\n",
    "    for i in range(MAX_CONTINUE):\n",
    "        answer = input(\"Do you want to skip? [yes(y)/no(n)]\" if i == 0 else \"Could not recognize your answer. Do you want to skip? [yes(y)/no(n)]\")\n",
    "        if answer in [\"yes\", \"Yes\", \"YES\", \"y\", \"Y\"]: \n",
    "            return\n",
    "        elif answer in [\"no\", \"No\", \"NO\", \"n\", \"N\"]: \n",
    "            !wget -P {GGUF_DIR} {url}\n",
    "            return\n",
    "\n",
    "download_default_model(\"https://huggingface.co/internlm/internlm2_5-7b-chat-gguf/resolve/main/internlm2_5-7b-chat-q6_k.gguf\")\n",
    "refresh_gguf_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88b6aed-48f4-4597-bb6d-e9e3b95202fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def format_to_html(context) -> str:\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import Python3Lexer\n",
    "    from pygments.formatters import HtmlFormatter\n",
    "    \n",
    "    def embed_image_to_tag(image_binary, width=None, height=None, alt=None) -> str:\n",
    "        import base64\n",
    "        encoded_image = base64.b64encode(image_binary).decode('utf-8')\n",
    "        html_image_tag = f'<img src=\"data:image/jpeg;base64,{encoded_image}\" '\n",
    "        if width:\n",
    "            html_image_tag += f'width=\"{width}\" '\n",
    "        if height:\n",
    "            html_image_tag += f'height=\"{height}\" '\n",
    "        if alt:\n",
    "            html_image_tag += f'alt=\"{alt}\" '\n",
    "        html_image_tag += '/>' # Closure\n",
    "        return html_image_tag\n",
    "\n",
    "    USER_MESSAGE_BG_COLOR = \"#BBFFBB\"\n",
    "    AI_MESSAGE_BG_COLOR = \"#FFEEBB\"\n",
    "    \n",
    "    messages: list[str] = []\n",
    "    for message in context.history():\n",
    "        text_template = \"\"\"<div style=\"background-color: {color}; min-width: 30px; width: fit-content; word-wrap: break-word; color: black; padding: 10px; border-radius: 0 20px 20px 20px;\">{content}</div>\"\"\"\n",
    "        header = lambda text: f'<div style=\"background-color: #999999; color: black; border-radius: 5px 5px 0 0\">{text}</div>'\n",
    "        \n",
    "        def render_user_item(item) -> str:\n",
    "            content = item[\"content\"]\n",
    "            name = f\"\"\"<div style=\"background-color: #999999; color: white; width: 15px; height: 15px; padding: 10px; border-radius: 5px 5px 0 0; text-align: center;\">{session.user_nickname[0]}</div>\"\"\"\n",
    "            body = text_template.format(content=md.convert(content), color=USER_MESSAGE_BG_COLOR)\n",
    "            return name + body\n",
    "            \n",
    "        def render_assistant_item(item) -> str:\n",
    "            name = f\"\"\"<div style=\"background-color: #999999; color: white; width: 15px; height: 15px; padding: 10px; border-radius: 5px 5px 0 0; text-align: center;\">{session.assistant_name[0]}</div>\"\"\"\n",
    "            \n",
    "            content = item[\"content\"]\n",
    "            tool = item.get(\"tool\")\n",
    "            \n",
    "            if not tool:\n",
    "                body = text_template.format(content=md.convert(content), color=AI_MESSAGE_BG_COLOR)\n",
    "                return name + body\n",
    "                \n",
    "            tool_name = tool.get(\"name\")\n",
    "            tool_action = tool.get(\"action\")\n",
    "            tool_input = tool.get(\"input\")\n",
    "            \n",
    "            if tool_action == \"call\":\n",
    "                if tool_name == tools.web_search.name:\n",
    "                    body = header(\"Search\") \n",
    "                    body += '<div style=\"background-color: #FFFFFF; color: black;\">' + tool_input + '</div>'\n",
    "                    body = text_template.format(content=body, color=AI_MESSAGE_BG_COLOR)\n",
    "                    return name + body\n",
    "                elif tool_name == tools.exec_python.name:\n",
    "                    body = header(\"Python\")\n",
    "                    body += highlight(tool_input, Python3Lexer(), HtmlFormatter())\n",
    "                    body = text_template.format(content=body, color=AI_MESSAGE_BG_COLOR)\n",
    "                    return name + body\n",
    "            \n",
    "        def render_tool_agent_item(item) -> str:\n",
    "            name = f\"\"\"<div style=\"background-color: #999999; color: white; width: 15px; height: 15px; padding: 10px; border-radius: 5px 5px 0 0; text-align: center;\">{session.assistant_name[0]}</div>\"\"\"\n",
    "            tool = item.get(\"tool\")\n",
    "            tool_name = tool.get(\"name\")\n",
    "            tool_action = tool.get(\"action\")\n",
    "            tool_output = tool.get(\"output\")\n",
    "            \n",
    "            if tool_name == tools.web_search.name:\n",
    "                references = tool_output.get(\"references\")\n",
    "                search_result = tool_output.get(\"search_result\")\n",
    "                body = header(\"Documents matched the query\")\n",
    "                body += '<div style=\"background-color: #FFFFFF; color: black;\">' + '<br>'.join([f'✅<a href=\"{url}\">︎{url[:50]}...</a>' for url in references]) + '</div>'\n",
    "                body = text_template.format(content=body, color=AI_MESSAGE_BG_COLOR)\n",
    "                return name + body\n",
    "            elif tool_name == tools.exec_python.name:\n",
    "                stdout = tool_output.get(\"stdout\")\n",
    "                image = tool_output.get(\"image\")\n",
    "                caption = tool_output.get(\"caption\")\n",
    "                body = header('Output')\n",
    "                if stdout:\n",
    "                    body += '<pre><code>' + stdout + '</code></pre>'\n",
    "                if caption:\n",
    "                    body += embed_image_to_tag(image_binary=image) + '</br>'\n",
    "                body = text_template.format(content=body, color=AI_MESSAGE_BG_COLOR)\n",
    "                return name + body\n",
    "                \n",
    "        def render_file_uploader_item(item) -> str:\n",
    "            name = f\"\"\"<div style=\"background-color: #999999; color: white; width: 15px; height: 15px; padding: 10px; border-radius: 5px 5px 0 0; text-align: center;\">{session.user_nickname[0]}</div>\"\"\"\n",
    "            image_output = item.get('image_output')\n",
    "            image_caption = item.get('caption')\n",
    "            body = \"\"\n",
    "            if image_output:\n",
    "                body += embed_image_to_tag(image_binary=image_output, width=300, alt=image_caption) + '</br>'\n",
    "            body = text_template.format(content=body, color=USER_MESSAGE_BG_COLOR)\n",
    "            return name + body\n",
    "\n",
    "        role = message['role']\n",
    "        if role == \"User\": messages.append(render_user_item(message))\n",
    "        elif role == session.assistant_name: messages.append(render_assistant_item(message))\n",
    "        elif role == TOOL_AGENT_NAME: messages.append(render_tool_agent_item(message))\n",
    "        elif role == FILE_UPLOADER_NAME: messages.append(render_file_uploader_item(message))\n",
    "        \n",
    "    return ''.join(messages)\n",
    "    \n",
    "def print_context():\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    HEIGHT, WIDTH = 1100, 800\n",
    "    \n",
    "    html_text = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <style>\n",
    "    #wrapper {{\n",
    "      display: flex;\n",
    "      flex-direction: column-reverse;\n",
    "      height: {HEIGHT}px;\n",
    "      width: {WIDTH}px;\n",
    "      overflow-y: scroll;\n",
    "    }}\n",
    "\n",
    "    /* Custom Scrollbar CSS */\n",
    "    #wrapper::-webkit-scrollbar {{\n",
    "      width: 5px;\n",
    "    }}\n",
    "\n",
    "    #wrapper::-webkit-scrollbar-track {{\n",
    "      background: #555555;\n",
    "    }}\n",
    "\n",
    "    #wrapper::-webkit-scrollbar-thumb {{\n",
    "      background: #888;\n",
    "    }}\n",
    "\n",
    "    #wrapper::-webkit-scrollbar-thumb:hover {{\n",
    "      background: #555;\n",
    "    }}\n",
    "    \n",
    "  </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div style=\"display: flex; align-items: flex-end;\">\n",
    "      <div id=\"wrapper\">\n",
    "        <div id=\"contents\">\n",
    "    {format_to_html(session.context)}\n",
    "        </div>\n",
    "      </div>\n",
    "    </div>\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    display(HTML(html_text))\n",
    "\n",
    "def update_display() -> None:\n",
    "    with session.out:\n",
    "        print_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f542ecf0-38fd-4648-8ff0-7a96fe714770",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TTS model loader\n",
    "\n",
    "def is_tts_model_dir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        return False\n",
    "    files_to_check = [\n",
    "        f\"{os.path.basename(path)}.safetensors\",\n",
    "        \"config.json\",\n",
    "        \"style_vectors.npy\",\n",
    "    ]\n",
    "    folder_files = os.listdir(path)\n",
    "    return all(file in folder_files for file in files_to_check)\n",
    "\n",
    "\n",
    "\n",
    "@session.debug.capture()\n",
    "def load_tts_models(model_path):\n",
    "    from style_bert_vits2.nlp import bert_models\n",
    "    from style_bert_vits2.constants import Languages\n",
    "    import gc; gc.collect()\n",
    "    \n",
    "    bert_models.load_model(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "    bert_models.load_tokenizer(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "    \n",
    "    return TTSModel(\n",
    "        model_path=os.path.join(model_path, f\"{os.path.basename(model_path)}.safetensors\"),\n",
    "        config_path=os.path.join(model_path, \"config.json\"),\n",
    "        style_vec_path=os.path.join(model_path, \"style_vectors.npy\"),\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def capture_model_selection_change():\n",
    "    while True:\n",
    "        selected = await wait_for_change(session.dropdown, \"value\")\n",
    "        session.tts_model = load_tts_models(os.path.join(TTS_ASSET_ROOT, selected))\n",
    "        session.debug.append_stdout(f\"tts_model({selected}) loaded.\\n\")\n",
    "\n",
    "tts_model_names = [item for item in os.listdir(TTS_ASSET_ROOT) if is_tts_model_dir(os.path.join(TTS_ASSET_ROOT, item))]\n",
    "if len(tts_model_names) != 0:\n",
    "    session.tts_model = load_tts_models(os.path.join(TTS_ASSET_ROOT, tts_model_names[0]))\n",
    "\n",
    "session.dropdown = widgets.Dropdown(\n",
    "    description=\"TTS model\", \n",
    "    options=tts_model_names, \n",
    "    value=tts_model_names[0]) if len(tts_model_names) != 0 else widgets.Dropdown(description=\"TTS model\", options=[],\n",
    "    layout=widgets.Layout(max_width=\"300px\", width=\"100%\"),\n",
    ")\n",
    "\n",
    "session.loop.create_task(capture_model_selection_change());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e78e503b-00ac-47ab-959f-a78a9d59b3df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def time_stamp() -> str:\n",
    "    import datetime\n",
    "    import time\n",
    "    now = datetime.datetime.now()\n",
    "        \n",
    "    weekday = ['Mon.', 'Tue', 'Wed.', 'Thu.', 'Fri.', 'Sat.', 'Sun.']\n",
    "    return \"{}-{:02}-{:02} {} {:02}:{:02}({})\".format(\n",
    "        now.year,\n",
    "        now.month,\n",
    "        now.day,\n",
    "        weekday[now.weekday()],\n",
    "        now.hour,\n",
    "        now.minute,\n",
    "        time.tzname[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c51affe-1021-4400-826f-590292fc4d41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_sbv2_dict(dict_path: str = \"sbv2_dict.json\") -> dict:\n",
    "    import json\n",
    "    if not os.path.isfile(dict_path): \n",
    "        return {}\n",
    "    with open(dict_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def assistant_speaks(text, autoplay: bool = False):\n",
    "    \"\"\"\n",
    "    A document describing current issue.\n",
    "    https://github.com/jupyter/notebook/issues/3468\n",
    "\n",
    "    According to this article, we cannot send messages larger than 10MB by default due to websocket message limit.\n",
    "    Binaries of long Wave tend to exceed this limit and hence causes undefined behavior. \n",
    "    It's okay to force users to unlock the limit, but for now, it's more convinient to convert the wave binary to compressed format like MP3.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from IPython.display import Audio\n",
    "    from pydub import AudioSegment\n",
    "    from scipy.io import wavfile\n",
    "    \n",
    "    if session.tts_model is None:\n",
    "        return\n",
    "\n",
    "    # Text modification\n",
    "    text = mixed2katakana(text)\n",
    "    text = re.sub(r'[(（].*?[)）]', '　', text)\n",
    "    text = replace_text(text, load_sbv2_dict())\n",
    "    if len(text) <= 0: return\n",
    "\n",
    "    # Generate audio as np.array\n",
    "    sampling_rate, wav = session.tts_model.infer(\n",
    "        text, \n",
    "        length=session.voice_length.value\n",
    "    )\n",
    "\n",
    "    # Convert to MP3\n",
    "    temp_wav_path = os.path.join(CACHE_DIR, \"temp.wav\")\n",
    "    temp_mp3_path = os.path.join(CACHE_DIR, \"temp.mp3\")\n",
    "    wavfile.write(temp_wav_path, sampling_rate, wav.astype(np.int16))\n",
    "    audio = AudioSegment.from_wav(temp_wav_path)\n",
    "    audio.export(temp_mp3_path, format='mp3')\n",
    "\n",
    "    return Audio(temp_mp3_path, autoplay=autoplay)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "@disable_uis\n",
    "def create_voice_action(sender=None):\n",
    "    try:\n",
    "        last_item = session.context.history()[-1]\n",
    "        text = last_item.get(\"content\")\n",
    "        with session.debug:\n",
    "            audio_widget = assistant_speaks(text)\n",
    "        session.voice_player.clear_output(wait=True)\n",
    "        with session.voice_player: \n",
    "            display(audio_widget)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96546328-7fa7-4fa8-9e6e-5a0b4a441bdb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize(sender=None) -> None:\n",
    "    session.context.reset()\n",
    "    session.field.value = ''\n",
    "    session.user_nickname_field.disabled = False\n",
    "    session.assistant_name_field.disabled = False\n",
    "    session.user_preamble.disabled = False\n",
    "    session.template_selector.disabled = False\n",
    "    set_buttons(disabled=False)\n",
    "    set_guessing_image(show=False)\n",
    "    tools.tool_selector.disabled = False\n",
    "    \n",
    "    session.context_window = 0\n",
    "\n",
    "    update_display()\n",
    "\n",
    "    from IPython.display import Audio\n",
    "    session.voice_player.clear_output(wait=True)\n",
    "    with session.voice_player:\n",
    "        display(Audio(b''))\n",
    "\n",
    "    session.initialized = True\n",
    "\n",
    "initialize()\n",
    "\n",
    "def start_session(sender=None) -> None:\n",
    "    if session.user_nickname_field.value == \"\":\n",
    "        session.user_nickname_field.value = DEFAULT_USER_NICKNAME\n",
    "    if session.assistant_name_field.value == \"\":\n",
    "        session.assistant_name_field.value = DEFAULT_ASSISTANT_NAME\n",
    "    if session.user_nickname_field.value == session.assistant_name_field.value:\n",
    "        shared_name = session.user_nickname_field.value\n",
    "        session.user_nickname_field.value = f\"{shared_name}_1\"\n",
    "        session.assistant_name_field.value = f\"{shared_name}_2\"\n",
    "    session.user_nickname = session.user_nickname_field.value\n",
    "    session.assistant_name = session.assistant_name_field.value\n",
    "    session.user_nickname_field.disabled = True\n",
    "    session.assistant_name_field.disabled = True\n",
    "    session.user_preamble.disabled = True\n",
    "    session.template_selector.disabled = True\n",
    "    session.login_time_stamp = time_stamp()\n",
    "    tools.tool_selector.disabled = True\n",
    "\n",
    "    session.initialized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f494e6af-52ff-47c6-b88b-96bfa070b7ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def interrupt_generation() -> None:\n",
    "    \"\"\"\n",
    "    Currently this do nothing since llama-cpp-python doesn't provide the functionality. \n",
    "    Related PRs:\n",
    "        https://github.com/abetlen/llama-cpp-python/pull/733\n",
    "        https://github.com/abetlen/llama-cpp-python/issues/599\n",
    "    \"\"\"\n",
    "    with session.debug: \n",
    "        print(\"Generation interrupted.\")\n",
    "\n",
    "\n",
    "        \n",
    "def predict_stream(force_direct_answer: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "        Returns:\n",
    "            dict: {\"tool\": tool , \"tool_input\": tool_input}\n",
    "    \"\"\"\n",
    "    from llama_cpp import LlamaGrammar\n",
    "    \n",
    "    params = session.generation_params\n",
    "    prefix = params.prefix.value\n",
    "\n",
    "    any_string = r'([^\\n]|\"\\n\")+'\n",
    "\n",
    "    \n",
    "    tools_gbnf = \"(\" + \"|\".join(\n",
    "        [f'\"<tool>{tool.name}</tool>\" \"<tool_input>\"{tool.grammar}\"</tool_input>\"' for tool in session.tools()]\n",
    "    ) + \")\"\n",
    "    tool_use_gbnf = f\"root ::= {tools_gbnf}\"\n",
    "\n",
    "    \n",
    "    direct_answer_gbnf = f'''root ::= \"<tool>\" name \"</tool>\" \"<tool_input>\" input \"</tool_input>\"\n",
    "name ::= \"direct_answer\"\n",
    "input ::= string\n",
    "string ::= {any_string}\n",
    "'''\n",
    "    \n",
    "    direct_answer_with_prefix_gbnf = f'''root ::= \"<tool>\" name \"</tool>\" \"<tool_input>{prefix}\" input \"</tool_input>\"\n",
    "name ::= \"direct_answer\"\n",
    "input ::= string\n",
    "string ::= {any_string}\n",
    "'''\n",
    "    grammar: LlamaGrammar\n",
    "    if force_direct_answer: \n",
    "        grammar = LlamaGrammar.from_string(direct_answer_gbnf)\n",
    "    elif prefix != \"\":\n",
    "        grammar = LlamaGrammar.from_string(direct_answer_with_prefix_gbnf)\n",
    "    else:\n",
    "        grammar = LlamaGrammar.from_string(tool_use_gbnf)\n",
    "    \n",
    "    prompt = prompt_builder.build()\n",
    "    \n",
    "    with session.debug:\n",
    "        print(f'prompt = \"\"\"{prompt}\"\"\"')\n",
    "        \n",
    "    streamer = session.model.stream(\n",
    "        input=prompt,\n",
    "        temperature=params.temperature.value,\n",
    "        top_p=params.top_p.value,\n",
    "        top_k=params.top_k.value,\n",
    "        frequency_penalty=params.frequency_penalty.value,\n",
    "        presence_penalty=params.presence_penalty.value,\n",
    "        repeat_penalty=params.repeat_penalty.value,\n",
    "        max_tokens=session.max_gen_tokens,\n",
    "        grammar=grammar,\n",
    "        stop=[\"</tool_input>\"]+prompt_builder.stops(),\n",
    "    )\n",
    "\n",
    "    def parse_tool_tag(text: str) -> str|None:\n",
    "        pattern = r\"<tool>({tools})</tool>\".format(tools='|'.join([tool.name for tool in session.tools()]))\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    def parse_unclosed_tool_input_tag(text: str) -> str:\n",
    "        pattern = re.compile(\"<tool_input>(.*)\", re.DOTALL)\n",
    "        match = re.search(pattern, text)\n",
    "        return match.group(1) if match else \"\"\n",
    "        \n",
    "    def rstrip_seq(text: str, seq: str) -> str:\n",
    "        for i in range(len(seq)):\n",
    "            incomplete_seq = seq[:len(seq) - i]\n",
    "            if text.endswith(incomplete_seq):\n",
    "                return text[:-len(incomplete_seq)]\n",
    "        return text\n",
    "        \n",
    "    def lstrip_seq(text: str, seq: str) -> str:\n",
    "        for i in range(len(seq) + 1):\n",
    "            incomplete_seq = seq[:len(seq) - i]\n",
    "            print(incomplete_seq)\n",
    "            if text.startswith(incomplete_seq):\n",
    "                return text[len(incomplete_seq):]\n",
    "        return text\n",
    "\n",
    "\n",
    "    text =  \"\"\n",
    "    for token in streamer:\n",
    "        text += token\n",
    "        tool = parse_tool_tag(text)\n",
    "        tool_input = parse_unclosed_tool_input_tag(text)\n",
    "        tool_input = rstrip_seq(tool_input, \"</tool_input>\")\n",
    "        if not params.append_prefix.value:\n",
    "            tool_input = lstrip_seq(tool_input, prefix)\n",
    "        yield {\"tool\": tool , \"tool_input\": tool_input.strip()}\n",
    "\n",
    "\n",
    "def predict_stream_with_display(force_direct_answer: bool = False):\n",
    "    try:\n",
    "        with session.debug:\n",
    "            for response in predict_stream(force_direct_answer):\n",
    "                if response[\"tool\"] in [tool.name for tool in tools.tools() if tool.name != tools.direct_answer.name]: \n",
    "                    session.context.push({\n",
    "                        \"role\": session.assistant_name,\n",
    "                        \"content\": \"\",\n",
    "                        \"tool\": {\n",
    "                            \"action\": \"call\",\n",
    "                            \"name\": response[\"tool\"],\n",
    "                            \"input\": response[\"tool_input\"],\n",
    "                        }\n",
    "                    })\n",
    "                else:\n",
    "                    session.context.push({\n",
    "                        \"role\": session.assistant_name, \n",
    "                        \"content\": response[\"tool_input\"],\n",
    "                    })\n",
    "                update_display()\n",
    "                session.context.force_pop_front()\n",
    "    except KeyboardInterrupt:\n",
    "        # This works fine in most of the cases, but if interruption occurs between push and pop, the behavior is undeifned.\n",
    "        interrupt_generation()\n",
    "    finally:\n",
    "        session.context.push_message(session.assistant_name, response[\"tool_input\"])\n",
    "        update_display()\n",
    "        session.context.force_pop_front()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfb718b3-29cc-4385-af56-9214ae3236bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def shift_context_by_item(n_items: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    Shift KV Cache by amount of n_items leaving system prompts.\n",
    "    Calling this function without decrement context_window result in failure of context shifting.\n",
    "\n",
    "    Implementation of StreamingLLM in oobabooga/text-generation-webui:\n",
    "    https://github.com/oobabooga/text-generation-webui/blob/main/modules/cache_utils.py#L24\n",
    "    According to this implementation, prefix (=n_sys_ppt_token) corresponds to the \"Attention Sinks\" in terms of StreamingLLM.\n",
    "    \n",
    "    Args:\n",
    "        n_items (int, default 1): Number of items to shift, but NOT the number of \"tokens\".\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from lib.infrastructure import kv_cache_seq_ltrim\n",
    "    \n",
    "    for i in range(n_items):\n",
    "        trimmed_history = session.context.history()[-session.context_window:]\n",
    "        oldest_item = trimmed_history[i]\n",
    "        \n",
    "        n_sys_ppt_tokens = session.model.token_count(prompt_builder.render_instruction()) + 1 # +1 for bos token.\n",
    "        n_oldest_ctx_tokens = session.model.token_count(prompt_builder.render_item(oldest_item))\n",
    "        \n",
    "        kv_cache_seq_ltrim(\n",
    "            model=session.model.llama, \n",
    "            n_keep=n_sys_ppt_tokens,\n",
    "            n_discard=n_oldest_ctx_tokens,\n",
    "        )\n",
    "        \n",
    "        with session.debug: \n",
    "            print(f\"{n_oldest_ctx_tokens} tokens discarded from position {n_sys_ppt_tokens}.\")\n",
    "            print(f\"discarded: {prompt_builder.render_item(oldest_item)}\")\n",
    "    \n",
    "\n",
    "def push_context(item, auto_shift_kv: bool) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        item: new item to push.\n",
    "        auto_shift_kv(bool): if true, shift KV-Cache and input_id if needed.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    session.context.push(item)\n",
    "\n",
    "    if not auto_shift_kv:\n",
    "        return\n",
    "\n",
    "    session.context_window += 1\n",
    "    \n",
    "    if session.model.token_count(prompt_builder.build()) < session.n_ctx - session.max_gen_tokens:\n",
    "        return\n",
    "        \n",
    "    while session.model.token_count(prompt_builder.build()) >= session.n_ctx - session.max_gen_tokens:\n",
    "        # The order matters since shift_context_by_item uses context_window internally.\n",
    "        # Dont decrement context_windows before calling shift_context_by_item.\n",
    "        if session.context_window >=2:\n",
    "            shift_context_by_item(1)\n",
    "            session.context_window -= 1\n",
    "        else:\n",
    "            session.context_window = 0\n",
    "            break\n",
    "\n",
    "\n",
    "def retrieve_latest_input(sender = None):\n",
    "    \"\"\"\n",
    "    Scheme for context management when retrieving item:\n",
    "              |        |\n",
    "    [0, 1, 2, 3, 4, 5, 6]\n",
    "              |     | <- shrink context window \n",
    "    [0, 1, 2, 3, 4, 5]\n",
    "    \"\"\"\n",
    "    session.context_window = max(session.context_window - 1, 0)\n",
    "\n",
    "    if len(session.context) <= 0:\n",
    "        session.field.value = ''\n",
    "        update_display()\n",
    "        return\n",
    "\n",
    "    last_item = session.context.force_pop_front(stringize=False)\n",
    "    if last_item['role'] == \"User\":\n",
    "        session.field.value = last_item['content']\n",
    "    update_display()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53f3e666-599f-4659-899f-cf6def4ba2fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model_not_loaded_error() -> None:\n",
    "    push_context({\n",
    "        \"role\": session.assistant_name, \n",
    "        \"content\": \"Model is not loaded yet. Please select and load a local model before inference.\"\n",
    "    }, auto_shift_kv=False)\n",
    "    update_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df533939-2566-4e73-ae24-f936d65bf9d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Main loop.\n",
    "@disable_uis\n",
    "def main(sender=None) -> None:\n",
    "    from lib.utils import reformat_python_code\n",
    "\n",
    "    def push_context_wrapper(item) -> None:\n",
    "        push_context(item, auto_shift_kv=True)\n",
    "        update_display()\n",
    "        \n",
    "    def export_log():\n",
    "        import datetime\n",
    "        filename = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S.txt\")\n",
    "        with open(os.path.join(LOG_DIR, filename), 'w') as f:\n",
    "           f.writelines(str(session.context))\n",
    "\n",
    "    if session.model == None:\n",
    "        model_not_loaded_error()\n",
    "        return\n",
    "\n",
    "    # Register user's input.\n",
    "    user_message = session.field.value\n",
    "    if session.model.token_count(user_message) >= MAX_USER_TOKENS: \n",
    "        return\n",
    "    if user_message == \"\": \n",
    "        return\n",
    "    if session.initialized:\n",
    "        start_session()\n",
    "    \n",
    "    session.field.value = ''\n",
    "    push_context_wrapper({\n",
    "        \"role\": \"User\", \n",
    "        \"content\": user_message\n",
    "    })\n",
    "\n",
    "    \n",
    "    response = predict_stream_with_display()\n",
    "    \n",
    "\n",
    "    if response[\"tool\"] == \"direct_answer\":\n",
    "        push_context_wrapper({\n",
    "            \"role\": session.assistant_name, \n",
    "            \"content\": response[\"tool_input\"],\n",
    "        })\n",
    "        export_log()\n",
    "        return\n",
    "\n",
    "    # This is special behavior of \"exec_python\".\n",
    "    if response[\"tool\"] == \"exec_python\":\n",
    "        response[\"tool_input\"] = reformat_python_code(response[\"tool_input\"])\n",
    "\n",
    "    # Register tool call in the context.\n",
    "    push_context_wrapper({\n",
    "        \"role\": session.assistant_name,\n",
    "        \"content\": \"\",\n",
    "        \"tool\": {\n",
    "            \"action\": \"call\",\n",
    "            \"name\": response[\"tool\"],\n",
    "            \"input\": response[\"tool_input\"],\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # Run tools.\n",
    "    push_context_wrapper({\n",
    "        \"role\": TOOL_AGENT_NAME,\n",
    "        \"content\": \"\",\n",
    "        \"tool\": {\n",
    "            \"action\": \"return\",\n",
    "            \"name\": response[\"tool\"],\n",
    "            \"output\": tools.run_tool(response[\"tool\"], response[\"tool_input\"]),\n",
    "        }\n",
    "    })\n",
    "        \n",
    "    # Add reaction to the tool results.\n",
    "    reaction = predict_stream_with_display(force_direct_answer=True)\n",
    "    push_context_wrapper({\n",
    "        \"role\": session.assistant_name, \n",
    "        \"content\": reaction[\"tool_input\"]\n",
    "    })\n",
    "\n",
    "    export_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7051be7f-182c-43e4-8f00-98e7955e7f2d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Multimodal\n",
    "\n",
    "def get_caption(img):\n",
    "    import io\n",
    "    from lib.multimodal import Florence2Large\n",
    "    from PIL import Image\n",
    "    \n",
    "    with session.debug:\n",
    "        if isinstance(img, bytes):\n",
    "            img = Image.open(io.BytesIO(img))\n",
    "            img = img.convert(\"RGB\")\n",
    "        \n",
    "        i2t = Florence2Large(use_accelerator=False)\n",
    "        i2t.load_model()\n",
    "        caption = i2t.get_caption(img)\n",
    "        \n",
    "    del i2t\n",
    "    return caption\n",
    "\n",
    "\n",
    "@disable_uis\n",
    "def generate_caption(args) -> None:\n",
    "    from PIL import Image\n",
    "    import io\n",
    "\n",
    "    if len(session.upload_file.value) == 0:\n",
    "        return\n",
    "\n",
    "    if session.model == None:\n",
    "        model_not_loaded_error()\n",
    "        session.upload_file.value = tuple()\n",
    "        return\n",
    "        \n",
    "    if session.initialized:\n",
    "        start_session()\n",
    "\n",
    "    try:\n",
    "        img_bytes = session.upload_file.value[0][\"content\"].tobytes()\n",
    "        img = Image.open(io.BytesIO(img_bytes))\n",
    "        img = img.convert(\"RGB\")\n",
    "        \n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\")\n",
    "        img_binary = buffer.getvalue()\n",
    "        \n",
    "        push_context({\n",
    "            \"role\": FILE_UPLOADER_NAME,\n",
    "            \"content\": \"\",\n",
    "            \"image_output\": img_binary,\n",
    "            \"caption\": get_caption(img),\n",
    "        }, auto_shift_kv=True)\n",
    "    finally:\n",
    "        session.upload_file.value = tuple()\n",
    "        update_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3547323b-eed7-43dd-973f-812718ecf408",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define UI actions.\n",
    "session.button.on_click(main)\n",
    "session.reset_button.on_click(initialize)\n",
    "session.retrieve.on_click(retrieve_latest_input)\n",
    "session.create_voice.on_click(create_voice_action)\n",
    "session.load_button.on_click(reload_model)\n",
    "session.unload_button.on_click(unload_model)\n",
    "session.reflesh_list_button.on_click(refresh_gguf_list)\n",
    "session.upload_file.observe(generate_caption, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9aae6c7-1574-4a98-892d-86eadd19773a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combine widgets.\n",
    "\n",
    "HBox = widgets.HBox\n",
    "\n",
    "    \n",
    "def show_top() -> None:\n",
    "    html = f\"\"\"<h1>Integrative LLM Chat UI for Jupyter</h1>\n",
    "<h3>Features</h3>\n",
    "1. voice synthesis</br>\n",
    "2. web search-based RAG</br>\n",
    "3. python env</br>\n",
    "4. image-to-text model-based image recognition</br>\n",
    "</br>\n",
    "To maximize the quality, I strongly recommend you to use models optimized for RAG/tools.</br>\n",
    "If you wanna use unsupported prompt template, define and register subclass of <code>PromptBuilderBase</code>.</br>\n",
    "<h3>Directories</h3>\n",
    "<code>./{GGUF_DIR}</code>: Directory to put GGUF models.</br>\n",
    "<code>./{AGENT_WORKING_DIR}</code>: Working directory when executing Python code.</br>\n",
    "<code>./{TTS_ASSET_ROOT}</code>: Directory to put TTS models.<br> \n",
    "(The subdirectory name containing the model should match the *.safetensors file name)</br>\n",
    "</br>\n",
    "\"\"\"\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def show_gguf_loader() -> None:\n",
    "    options = session.llama_cpp_options\n",
    "    \n",
    "    def output_memory_usage(output_widget: widgets.Output) -> None:\n",
    "        from threading import Thread\n",
    "    \n",
    "        def job() -> None:\n",
    "            from psutil import virtual_memory\n",
    "            import time\n",
    "            nonlocal output_widget\n",
    "            \n",
    "            while True:\n",
    "                time.sleep(2)\n",
    "                prop = virtual_memory().percent/100\n",
    "                bar_num = int(20*prop)\n",
    "                bar = \"[\" + \"|\"*bar_num + \" \"*(20-bar_num) + \"]\"\n",
    "                output_widget.outputs = ({\n",
    "                    'name': 'stdout', \n",
    "                    'text': \"RAM \" + bar + f\"{virtual_memory().percent}%\", \n",
    "                    'output_type': 'stream'\n",
    "                },)\n",
    "    \n",
    "        th = Thread(target=job)\n",
    "        th.start()\n",
    "\n",
    "    mem_viewer = widgets.Output()\n",
    "    output_memory_usage(mem_viewer)\n",
    "    \n",
    "    display(\n",
    "        HBox([options.define_n_ctx, options.define_max_gen_tokens, options.n_gpu_layers]),\n",
    "        HBox([options.flash_attention, options.quantize_kv]),\n",
    "        HBox([options.gguf_selector, session.load_button, session.unload_button, session.reflesh_list_button]),\n",
    "        session.active_gguf_viewer,\n",
    "        mem_viewer,\n",
    "    )\n",
    "\n",
    "def show_session_options() -> None:\n",
    "    display(\n",
    "        HBox([session.template_selector,session.streamingllm]),\n",
    "        HBox([session.user_nickname_field, session.assistant_name_field]),\n",
    "        tools.tool_selector.display(),\n",
    "        HBox([session.user_preamble]),\n",
    "    )\n",
    "\n",
    "def show_main_context_window() -> None:\n",
    "    display(session.out)\n",
    "\n",
    "def show_generation_params() -> None:\n",
    "    params = session.generation_params\n",
    "    \n",
    "    display(\n",
    "        HBox([params.temperature, params.top_k]),\n",
    "        HBox([params.top_p, params.frequency_penalty]),\n",
    "        HBox([params.presence_penalty, params.repeat_penalty]),\n",
    "        HBox([params.prefix, params.append_prefix]),\n",
    "    )\n",
    "\n",
    "def show_input_field() -> None:\n",
    "    display(\n",
    "        HBox([session.field, session.button, session.guessing_image]),\n",
    "        HBox([session.upload_file, session.retrieve, session.reset_button]),\n",
    "    )\n",
    "\n",
    "def show_voice_synthesis() -> None:\n",
    "    display(\n",
    "        session.voice_player,\n",
    "        HBox([session.dropdown, session.voice_length]),\n",
    "        HTML(\"Edit <code>sbv2_dict.json</code> to configure words replacement.\"),\n",
    "        session.create_voice,\n",
    "    )\n",
    "\n",
    "def show_debug_window() -> None:\n",
    "    clear = widgets.Button(description=\"Clear\")\n",
    "    def clear_debug_window(sender) -> None:\n",
    "        session.debug.clear_output()\n",
    "    clear.on_click(clear_debug_window)\n",
    "    display(session.debug, clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9efe589b-72d7-48c6-beb0-ba5ad0b2bc45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06faa6ed95d641b09da0e4aa74707315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(), Output(), Output(), Output()), titles=('About This App', 'GGUF Loader', 'Session…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4722f94f2e4b6d9e930f0610120a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679119181c6244ddaf6391e430f3a80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(),), titles=('Generation Params',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53a2a05b0f445ca91f1bd73297b2ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value='', layout=Layout(height='auto', max_width='700px', width='100%'), placeholder='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d76ab2aecb4622a739f0d2da8e1cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FileUpload(value=(), accept='.png,.jpg,.jpeg,.gif,.bmp', description='Image', layout=Layout(wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7067be08904b6da0ea34fb6bd54af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(Output(),), titles=('Text-to-Speech',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ignore_warnings()\n",
    "\n",
    "top_output = widgets.Output()\n",
    "gguf_load_output = widgets.Output()\n",
    "session_option_output = widgets.Output()\n",
    "generation_params_output = widgets.Output()\n",
    "voice_synthesis_output = widgets.Output()\n",
    "debug_output = widgets.Output()\n",
    "\n",
    "with top_output: show_top()\n",
    "with gguf_load_output: show_gguf_loader()\n",
    "with session_option_output: show_session_options()\n",
    "with generation_params_output: show_generation_params()\n",
    "with voice_synthesis_output: show_voice_synthesis()\n",
    "with debug_output: show_debug_window()\n",
    "\n",
    "\n",
    "display(widgets.Accordion(\n",
    "    children=[top_output, gguf_load_output, session_option_output, debug_output], \n",
    "    titles=[\"About This App\", \"GGUF Loader\", \"Session Options\", \"Debug\"]\n",
    "))\n",
    "show_main_context_window()\n",
    "display(widgets.Accordion(\n",
    "    children=[generation_params_output], \n",
    "    titles=[\"Generation Params\"]\n",
    "))\n",
    "show_input_field()\n",
    "display(widgets.Accordion(\n",
    "    children=[voice_synthesis_output], \n",
    "    titles=[\"Text-to-Speech\"]\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2-clone",
   "language": "python",
   "name": "llama2-clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
