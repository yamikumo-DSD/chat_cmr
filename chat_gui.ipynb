{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8b1b04-e2ed-4184-be52-0584f617ab9d",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "1. Jupyter must be installed.<br>\n",
    "<code>\\$ pip install jupyterlab</code><br>\n",
    "2. Activate ipywidgets by one of following commands:<br>\n",
    "    2-1. jupyter-lab<br>\n",
    "<code>\\$ jupyter labextension install @jupyter-widgets/jupyterlab-manager</code><br>\n",
    "    2-2. jupyter notebook / Google Colab<br>\n",
    "<code>\\$ jupyter nbextension enable --py widgetsnbextension</code>\n",
    "3. Install dependencies.<br>\n",
    "<code>\\$ pip install -r requirements.txt</code>\n",
    "\n",
    "## â‡©â‡© RUN ALL CELLS BELOW â‡©â‡©\n",
    "Run cells all the way down to the bottom!<br>\n",
    "<font color=#FFFF00>**CAUTION!**</font> First run will take time to download a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed8c855-4eab-4fd6-8d95-2149dbff4a81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "ignore_warnings = lambda : simplefilter('ignore')\n",
    "ignore_warnings()\n",
    "\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import dataclasses\n",
    "from style_bert_vits2.tts_model import TTSModel\n",
    "\n",
    "from IPython.display import clear_output, HTML\n",
    "from ipywidgets import widgets\n",
    "\n",
    "import lib.markdown as md\n",
    "from lib.infrastructure import ForgetableContext, LlamaCpp\n",
    "from lib.uis import wait_for_change\n",
    "from lib.utils import now, mixed2katakana, get, replace_text\n",
    "\n",
    "import session_states as session\n",
    "from global_settings import *\n",
    "from prompt_builders import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700d3d8b-a24e-4c23-83c5-50de61ef244f",
   "metadata": {},
   "source": [
    "### Remove one cell below to disable default model download at first run.\n",
    "This process will be automatically skipped if a directory <code>./ggufs</code> is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d203472-ff36-4072-99c5-e97aa41cec0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if len(session.ggufs) == 0:\n",
    "    # IMO, Llama-3-8B is the minimum requirement that can run Python/RAG/tools.\n",
    "    # I don't believe Llama-2-based small model can run this appðŸ¤”\n",
    "    print(f\"Directory \\\"{GGUF_DIR}\\\" is empty. Downloading default model.\")\n",
    "    LLAMA_3_8B_GGUF_HF_URL = \"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q5_K_M.gguf\"\n",
    "    !wget -P {GGUF_DIR} {LLAMA_3_8B_GGUF_HF_URL}\n",
    "    session.ggufs.append(\"Meta-Llama-3-8B-Instruct.Q5_K_M.gguf\")\n",
    "    session.llama_cpp_options.gguf_selector.options = session.ggufs\n",
    "    session.llama_cpp_options.gguf_selector.value = session.ggufs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fb6bf1-2861-44ba-a933-c6114d7e3529",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reload_instruction(instruction_file_path: str):\n",
    "    instructions = \"\"\n",
    "    with open(instruction_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        instructions = ''.join(lines)\n",
    "        \n",
    "    return instructions\n",
    "\n",
    "prompt_builder = Llama3PromptBuilder(\n",
    "    reload_instruction(\"system_prompt_template/l3_sys_ppt_gen_16.txt\"), \n",
    "    session.context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b25fa-acdf-4ba4-9522-ea8979e90ff7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Updates GUIs.\n",
    "    \n",
    "def get_bigger(args):        \n",
    "    nls = session.field.value.count('\\n')\n",
    "    session.field.rows = nls + 1 if nls >= 1 else 2\n",
    "session.field.observe(get_bigger, 'value')\n",
    "\n",
    "def change_prompt_builder(args):\n",
    "    global prompt_builder\n",
    "    context = session.context\n",
    "    prompt_builder = {\n",
    "        \"Command R Template\": CommandRPromptBuilder(reload_instruction(\"./system_prompt_template/cmdr_sys_ppt_gen_16.txt\"), context),\n",
    "        \"Llama-3 Template\": Llama3PromptBuilder(reload_instruction(\"./system_prompt_template/l3_sys_ppt_gen_16.txt\"), context),\n",
    "        \"ChatML Template\": ChatMLPromptBuilder(reload_instruction(\"./system_prompt_template/chatml_sys_ppt_gen_16.txt\"), context),\n",
    "    }[session.template_selector.value]\n",
    "\n",
    "session.template_selector.observe(change_prompt_builder, \"value\")\n",
    "\n",
    "\n",
    "def set_guessing_image(show: bool) -> None:\n",
    "    if not session.guessing_image:\n",
    "        with open('images/guessing.gif' if show else 'images/empty.png', 'rb') as f:\n",
    "            session.guessing_image = widgets.Image(value=f.read(), width=50, height=50)\n",
    "        \n",
    "    with open('images/guessing.gif' if show else 'images/empty.png', 'rb') as f:\n",
    "        session.guessing_image.value = f.read() \n",
    "        session.guessing_image.width = 50\n",
    "        session.guessing_image.height = 50\n",
    "\n",
    "set_guessing_image(False)\n",
    "\n",
    "\n",
    "def set_buttons(disabled: bool) -> None:\n",
    "    for b in session.buttons:\n",
    "        b.disabled = disabled\n",
    "\n",
    "def disable_uis(func):\n",
    "    from functools import wraps\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        set_buttons(disabled=True)\n",
    "        set_guessing_image(True)\n",
    "        result = func(*args, **kwargs)\n",
    "        set_guessing_image(False)\n",
    "        set_buttons(disabled=False)\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209f7c2-c82e-428d-b2c7-9d3af188195f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def _load_model(gguf_path: str) -> None:\n",
    "    assert session.n_ctx > MAX_GENERATION_TOKENS\n",
    "    session.n_ctx = session.llama_cpp_options.define_n_ctx.value\n",
    "\n",
    "    try:\n",
    "        session.model = LlamaCpp(\n",
    "            model_path=gguf_path,\n",
    "            n_gpu_layers=session.llama_cpp_options.n_gpu_layers.value,\n",
    "            n_batch=1024,\n",
    "            n_ctx=session.n_ctx,\n",
    "            use_mlock=True,\n",
    "            flash_attn=session.llama_cpp_options.flash_attention.value,\n",
    "            verbose=False,\n",
    "            embedding=False,\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        with session.debug:\n",
    "            print(e)\n",
    "    \n",
    "def _unload_model() -> None:\n",
    "    import gc\n",
    "    import llama_cpp\n",
    "\n",
    "    if session.model == None:\n",
    "        return\n",
    "\n",
    "    # Explicitly free the model.\n",
    "    # https://github.com/abetlen/llama-cpp-python/pull/1513\n",
    "    # session.model.llama.close()\n",
    "    \n",
    "    session.model = None\n",
    "    gc.collect()\n",
    "    \n",
    "@disable_uis\n",
    "def reload_model(sender=None) -> None:\n",
    "    _unload_model()\n",
    "    _load_model(os.path.join(GGUF_DIR, session.llama_cpp_options.gguf_selector.value))\n",
    "    initialize()\n",
    "    \n",
    "@disable_uis\n",
    "def unload_model(sender=None) -> None:\n",
    "    _unload_model()\n",
    "    initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b6aed-48f4-4597-bb6d-e9e3b95202fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def format_to_html(context) -> str:\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import Python3Lexer\n",
    "    from pygments.formatters import HtmlFormatter\n",
    "    \n",
    "    def embed_image_to_tag(image_binary) -> str:\n",
    "        import base64\n",
    "        encoded_image = base64.b64encode(image_binary).decode('utf-8')\n",
    "        html_image_tag = f'<img src=\"data:image/jpeg;base64,{encoded_image}\" />'\n",
    "        return html_image_tag\n",
    "\n",
    "    USER_MESSAGE_BG_COLOR = \"#BBFFBB\"\n",
    "    AI_MESSAGE_BG_COLOR = \"#FFEEBB\"\n",
    "    \n",
    "    messages: list[str] = []\n",
    "    for message in context.history():\n",
    "        role = message['role']\n",
    "        content = message['content']\n",
    "\n",
    "\n",
    "        name: str\n",
    "        text: str\n",
    "        text_template = \"\"\"<div style=\"background-color: {color}; word-wrap: break-word; color: black; padding: 10px; border-radius: 20px;\">{content}</div>\"\"\"\n",
    "        \n",
    "        if role == \"User\":\n",
    "            text = md.convert(content)\n",
    "            text = text_template.format(content=text, color=USER_MESSAGE_BG_COLOR)\n",
    "        else:\n",
    "            text = md.convert(content)\n",
    "\n",
    "            references = get(message, 'references')\n",
    "            code = get(message, 'code')\n",
    "            code_output = get(message, 'code_output')\n",
    "            image_output = get(message, 'image_output')\n",
    "            search_result = get(message, 'search_result')\n",
    "            search_query = get(message, 'search_query')\n",
    "\n",
    "            header = lambda text: f'<div style=\"background-color: #999999; color: black;\">{text}</div>'\n",
    "            if search_query:\n",
    "                text += header(\"Google\")\n",
    "                text += '<div style=\"background-color: #FFFFFF; color: black;\">' + search_query + '</div>'\n",
    "            if search_result:\n",
    "                # The search result itself is hidden.\n",
    "                pass\n",
    "            if references:\n",
    "                text += header(\"Documents matched the query\")\n",
    "                text += '<div style=\"background-color: #FFFFFF; color: black;\">' + '<br>'.join([f'âœ…<a href=\"{url}\">ï¸Ž{url[:50]}...</a>' for url in references]) + '</div>'\n",
    "            if code:\n",
    "                text += header(\"Python\")\n",
    "                text += highlight(code, Python3Lexer(), HtmlFormatter())\n",
    "            if code_output:\n",
    "                text += header('Output')\n",
    "                text += '<pre><code>' + code_output + '</code></pre>'\n",
    "            if image_output:\n",
    "                text += '</br>' + embed_image_to_tag(image_binary=image_output) + '</br>'\n",
    "\n",
    "            text = text_template.format(content=text, color=AI_MESSAGE_BG_COLOR)\n",
    "\n",
    "        if role == \"User\":\n",
    "            name = f\"\"\"<div style=\"background-color: #999999; color: white; width: 15px; height: 15px; padding: 10px; border-radius: 5px; text-align: center;\">{session.user_nickname[0]}</div>\"\"\"\n",
    "        elif role == session.assistant_name:\n",
    "            name = f\"\"\"<div style=\"background-color: #999999; color: white; width: 15px; height: 15px; padding: 10px; border-radius: 5px; text-align: center;\">{session.assistant_name[0]}</div>\"\"\"\n",
    "        else:\n",
    "            name = \"</br>\"\n",
    "        messages.append(f'{name}{text}')\n",
    "        \n",
    "    return ''.join(messages)\n",
    "    \n",
    "def print_context():\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    HEIGHT, WIDTH = 1100, 800\n",
    "    \n",
    "    html_text = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "\n",
    "<head>\n",
    "  <meta charset=\"utf-8\" />\n",
    "  <style>\n",
    "    #wrapper {{\n",
    "      display: flex;\n",
    "      flex-direction: column-reverse;\n",
    "      height: {HEIGHT}px;\n",
    "      width: {WIDTH}px;\n",
    "      overflow-y: scroll;\n",
    "    }}\n",
    "\n",
    "    /* Custom Scrollbar CSS */\n",
    "    #wrapper::-webkit-scrollbar {{\n",
    "      width: 10px;\n",
    "    }}\n",
    "\n",
    "    #wrapper::-webkit-scrollbar-track {{\n",
    "      background: #f1f1f1;\n",
    "    }}\n",
    "\n",
    "    #wrapper::-webkit-scrollbar-thumb {{\n",
    "      background: #888;\n",
    "    }}\n",
    "\n",
    "    #wrapper::-webkit-scrollbar-thumb:hover {{\n",
    "      background: #555;\n",
    "    }}\n",
    "    \n",
    "  </style>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <div style=\"display: flex; align-items: flex-end;\">\n",
    "      <div id=\"wrapper\">\n",
    "        <div id=\"contents\">\n",
    "    {format_to_html(session.context)}\n",
    "        </div>\n",
    "      </div>\n",
    "    </div>\n",
    "</body>\n",
    "\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "    display(HTML(html_text))\n",
    "\n",
    "def update_display() -> None:\n",
    "    with session.out:\n",
    "        print_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542ecf0-38fd-4648-8ff0-7a96fe714770",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TTS model loader\n",
    "\n",
    "def is_tts_model_dir(path):\n",
    "    if not os.path.isdir(path):\n",
    "        return False\n",
    "    files_to_check = [\n",
    "        f\"{os.path.basename(path)}.safetensors\",\n",
    "        \"config.json\",\n",
    "        \"style_vectors.npy\",\n",
    "    ]\n",
    "    folder_files = os.listdir(path)\n",
    "    return all(file in folder_files for file in files_to_check)\n",
    "\n",
    "\n",
    "\n",
    "@session.debug.capture()\n",
    "def load_tts_models(model_path):\n",
    "    from style_bert_vits2.nlp import bert_models\n",
    "    from style_bert_vits2.constants import Languages\n",
    "    import gc; gc.collect()\n",
    "    \n",
    "    bert_models.load_model(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "    bert_models.load_tokenizer(Languages.JP, \"ku-nlp/deberta-v2-large-japanese-char-wwm\")\n",
    "    \n",
    "    return TTSModel(\n",
    "        model_path=os.path.join(model_path, f\"{os.path.basename(model_path)}.safetensors\"),\n",
    "        config_path=os.path.join(model_path, \"config.json\"),\n",
    "        style_vec_path=os.path.join(model_path, \"style_vectors.npy\"),\n",
    "        device=\"cpu\"\n",
    "    )\n",
    "\n",
    "\n",
    "async def capture_model_selection_change():\n",
    "    while True:\n",
    "        selected = await wait_for_change(session.dropdown, \"value\")\n",
    "        session.tts_model = load_tts_models(os.path.join(TTS_ASSET_ROOT, selected))\n",
    "        session.debug.append_stdout(f\"tts_model({selected}) loaded.\\n\")\n",
    "\n",
    "tts_model_names = [item for item in os.listdir(TTS_ASSET_ROOT) if is_tts_model_dir(os.path.join(TTS_ASSET_ROOT, item))]\n",
    "if len(tts_model_names) != 0:\n",
    "    session.tts_model = load_tts_models(os.path.join(TTS_ASSET_ROOT, tts_model_names[0]))\n",
    "session.dropdown = widgets.Dropdown(description=\"TTS model\", options=tts_model_names, value=tts_model_names[0]) if len(tts_model_names) != 0 else widgets.Dropdown(description=\"TTS model\", options=[])\n",
    "session.loop.create_task(capture_model_selection_change());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e503b-00ac-47ab-959f-a78a9d59b3df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def export_log():\n",
    "    import datetime\n",
    "    filename = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S.txt\")\n",
    "    with open(os.path.join(LOG_DIR, filename), 'w') as f:\n",
    "       f.writelines(str(session.context))\n",
    "        \n",
    "def time_stamp() -> str:\n",
    "    import datetime\n",
    "    import time\n",
    "    now = datetime.datetime.now()\n",
    "        \n",
    "    weekday = ['Mon.', 'Tue', 'Wed.', 'Thu.', 'Fri.', 'Sat.', 'Sun.']\n",
    "    return \"{}-{:02}-{:02} {} {:02}:{:02}({})\".format(\n",
    "        now.year,\n",
    "        now.month,\n",
    "        now.day,\n",
    "        weekday[now.weekday()],\n",
    "        now.hour,\n",
    "        now.minute,\n",
    "        time.tzname[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51affe-1021-4400-826f-590292fc4d41",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_sbv2_dict(dict_path: str = \"sbv2_dict.json\") -> dict:\n",
    "    import json\n",
    "    if not os.path.isfile(dict_path): \n",
    "        return {}\n",
    "    with open(dict_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def assistant_speaks(text) -> None:\n",
    "    if session.tts_model is None:\n",
    "        return\n",
    "    \n",
    "    text = mixed2katakana(text)\n",
    "    text = re.sub(r'[(ï¼ˆ].*?[)ï¼‰]', 'ã€€', text)\n",
    "\n",
    "    text = replace_text(text, load_sbv2_dict())\n",
    "    \n",
    "\n",
    "    try:\n",
    "        from IPython.display import Audio\n",
    "        if len(text) > 0:\n",
    "            with session.debug: \n",
    "                sr, wav = session.tts_model.infer(\n",
    "                    text, \n",
    "                    length=session.voice_length.value\n",
    "                )\n",
    "        else:\n",
    "            return\n",
    "        audio = Audio(wav, rate=sr, autoplay=False)\n",
    "        session.voice_player.clear_output(wait=True)\n",
    "        with session.voice_player: display(audio)\n",
    "    except BaseException as e:\n",
    "        with session.debug: print(e)\n",
    "            \n",
    "@disable_uis\n",
    "def create_voice_action(sender=None):\n",
    "    try:\n",
    "        text = get(session.context.history()[-1], \"content\")\n",
    "        assistant_speaks(text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96546328-7fa7-4fa8-9e6e-5a0b4a441bdb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def initialize(sender=None) -> None:\n",
    "    session.context.reset()\n",
    "    session.field.value = ''\n",
    "    session.user_nickname_field.disabled = False\n",
    "    session.assistant_name_field.disabled = False\n",
    "    session.user_preamble.disabled = False\n",
    "    session.template_selector.disabled = False\n",
    "    set_buttons(disabled=False)\n",
    "    set_guessing_image(show=False)\n",
    "    \n",
    "    session.login_time_stamp = time_stamp()\n",
    "    session.context_window = 0\n",
    "\n",
    "    update_display()\n",
    "\n",
    "    from IPython.display import Audio\n",
    "    session.voice_player.clear_output(wait=True)\n",
    "    with session.voice_player:\n",
    "        display(Audio(b''))\n",
    "\n",
    "initialize()\n",
    "\n",
    "def submit_names_action(sender=None) -> None:\n",
    "    if session.user_nickname_field.value == \"\":\n",
    "        session.user_nickname_field.value = DEFAULT_USER_NICKNAME\n",
    "    if session.assistant_name_field.value == \"\":\n",
    "        session.assistant_name_field.value = DEFAULT_ASSISTANT_NAME\n",
    "    if session.user_nickname_field.value == session.assistant_name_field.value:\n",
    "        shared_name = session.user_nickname_field.value\n",
    "        session.user_nickname_field.value = f\"{shared_name}_1\"\n",
    "        session.assistant_name_field.value = f\"{shared_name}_2\"\n",
    "    session.user_nickname = session.user_nickname_field.value\n",
    "    session.assistant_name = session.assistant_name_field.value\n",
    "    session.user_nickname_field.disabled = True\n",
    "    session.assistant_name_field.disabled = True\n",
    "    session.user_preamble.disabled = True\n",
    "    session.template_selector.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363299d-8084-4800-ab11-623761e4e87e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def interrupt_generation() -> None:\n",
    "    \"\"\"\n",
    "    Currently this do nothing since llama-cpp-python doesn't provide the functionality. \n",
    "    Related PRs:\n",
    "        https://github.com/abetlen/llama-cpp-python/pull/733\n",
    "        https://github.com/abetlen/llama-cpp-python/issues/599\n",
    "    \"\"\"\n",
    "    with session.debug: \n",
    "        print(\"Generation interrupted.\")\n",
    "\n",
    "def predict_stream(additional_stop_tokens: list[str] = []):\n",
    "    prompt = prompt_builder.build()\n",
    "\n",
    "    generation_params = session.generation_params\n",
    "    \n",
    "    streamer = session.model.stream(\n",
    "        input=prompt,\n",
    "        temperature=generation_params.temperature.value,\n",
    "        top_p=generation_params.top_p.value,\n",
    "        top_k=generation_params.top_k.value,\n",
    "        frequency_penalty=generation_params.frequency_penalty.value,\n",
    "        presence_penalty=generation_params.presence_penalty.value,\n",
    "        repeat_penalty=generation_params.repeat_penalty.value,\n",
    "        max_tokens=MAX_GENERATION_TOKENS,\n",
    "        stop=additional_stop_tokens\n",
    "    )\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'(.*?```\\n?({tools}).*?```)'.format(tools='|'.join([tool.name for tool in session.tools])), \n",
    "        re.DOTALL\n",
    "    )\n",
    "\n",
    "    output = ''\n",
    "    for token in streamer:\n",
    "        output += token\n",
    "        \n",
    "        match = re.search(pattern, output)\n",
    "        if match:\n",
    "            output = match.group(1)\n",
    "            yield output\n",
    "            interrupt_generation()\n",
    "            break \n",
    "        else:\n",
    "            yield output.strip()\n",
    "        \n",
    "        \n",
    "\n",
    "def predict_stream_with_display(additional_stop_tokens: list[str] = []):\n",
    "    try:\n",
    "        with session.debug:\n",
    "            for output in predict_stream(additional_stop_tokens):\n",
    "                session.context.push_message(session.assistant_name, output)\n",
    "                update_display()\n",
    "                session.context.force_pop_front()\n",
    "    except KeyboardInterrupt:\n",
    "        # This works fine in most of the cases, but if interruption occurs between push and pop, the behavior is undeifned.\n",
    "        interrupt_generation()\n",
    "    finally:\n",
    "        session.context.push_message(session.assistant_name, output)\n",
    "        update_display()\n",
    "        session.context.force_pop_front()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb718b3-29cc-4385-af56-9214ae3236bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def shift_context_by_item(n_items: int = 1) -> None:\n",
    "    \"\"\"\n",
    "    Shift KV Cache by amount of n_items leaving system prompts.\n",
    "    Calling this function without decrement context_window result in failure of context shifting.\n",
    "\n",
    "    Implementation of StreamingLLM in oobabooga/text-generation-webui:\n",
    "    https://github.com/oobabooga/text-generation-webui/blob/main/modules/cache_utils.py#L24\n",
    "    According to this implementation, prefix (=n_sys_ppt_token) corresponds to the \"Attention Sinks\" in terms of StreamingLLM.\n",
    "    \n",
    "    Args:\n",
    "        n_items (int, default 1): Number of items to shift, but NOT the number of \"tokens\".\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from lib.infrastructure import kv_cache_seq_ltrim\n",
    "    \n",
    "    for i in range(n_items):\n",
    "        trimmed_history = session.context.history()[-session.context_window:]\n",
    "        oldest_item = trimmed_history[i]\n",
    "        \n",
    "        n_sys_ppt_tokens = session.model.token_count(prompt_builder.render_instruction()) + 1 # +1 for bos token.\n",
    "        n_oldest_ctx_tokens = session.model.token_count(prompt_builder.render_item(oldest_item))\n",
    "        \n",
    "        kv_cache_seq_ltrim(\n",
    "            model=session.model.llama, \n",
    "            n_keep=n_sys_ppt_tokens,\n",
    "            n_discard=n_oldest_ctx_tokens,\n",
    "        )\n",
    "        \n",
    "        with session.debug: \n",
    "            print(f\"{n_oldest_ctx_tokens} tokens discarded from position {n_sys_ppt_tokens}.\")\n",
    "            print(f\"discarded: {prompt_builder.render_item(oldest_item)}\")\n",
    "    \n",
    "\n",
    "def push_context(item, auto_shift_kv: bool) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        item: new item to push.\n",
    "        auto_shift_kv(bool): if true, shift KV-Cache and input_id if needed.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    session.context.push(item)\n",
    "\n",
    "    if not auto_shift_kv:\n",
    "        return\n",
    "\n",
    "    session.context_window += 1\n",
    "    \n",
    "    if session.model.token_count(prompt_builder.build()) < session.n_ctx - MAX_GENERATION_TOKENS:\n",
    "        return\n",
    "        \n",
    "    while session.model.token_count(prompt_builder.build()) >= session.n_ctx - MAX_GENERATION_TOKENS:\n",
    "        # The order matters since shift_context_by_item uses context_window internally.\n",
    "        # Dont decrement context_windows before calling shift_context_by_item.\n",
    "        shift_context_by_item(1)\n",
    "        session.context_window -= 1\n",
    "        if session.context_window == 0: break\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_latest_input(sender = None):\n",
    "    \"\"\"\n",
    "    Scheme for context management when retrieving item:\n",
    "              |        |\n",
    "    [0, 1, 2, 3, 4, 5, 6]\n",
    "              |     | <- shrink context window \n",
    "    [0, 1, 2, 3, 4, 5]\n",
    "    \"\"\"\n",
    "    session.context_window = max(session.context_window - 1, 0)\n",
    "\n",
    "    if len(session.context) <= 0:\n",
    "        session.field.value = ''\n",
    "        update_display()\n",
    "        return\n",
    "\n",
    "    last_item = session.context.force_pop_front(stringize=False)\n",
    "    if last_item['role'] == \"User\":\n",
    "        session.field.value = last_item['content']\n",
    "    update_display()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554cb9b-b3ae-4a67-8d0c-46e522f7be5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def python_chain(python_code) -> None:\n",
    "    if python_code == \"\":\n",
    "        push_context({\n",
    "            \"role\": PYTHON_RUNTIME_NAME,\n",
    "            \"content\": \"Empty code is not allowed.\",\n",
    "        }, auto_shift_kv=True)\n",
    "        update_display()\n",
    "        return\n",
    "\n",
    "        \n",
    "    # Run code.\n",
    "    session.py.unset(keep_locals=True)\n",
    "    session.py.run(python_code)\n",
    "    _, code_output, image_output = session.py.result()\n",
    "\n",
    "    # Register into context.\n",
    "    push_context({\n",
    "        \"role\": PYTHON_RUNTIME_NAME,\n",
    "        \"content\": \"\",\n",
    "        \"code_output\": code_output.strip() if code_output else \"Empty stdout/stderr.\",\n",
    "        \"image_output\": image_output,\n",
    "    }, auto_shift_kv=True)\n",
    "    update_display()\n",
    "\n",
    "\n",
    "\n",
    "def search_chain(search_query: str) -> None:\n",
    "    # To discard dialogs of first downloading the model.\n",
    "    with session.debug:\n",
    "        from lib.rag import pick_relevant_web_documents, MultilingualE5Small\n",
    "\n",
    "    if search_query == \"\":\n",
    "        push_context({\n",
    "            \"role\": SEARCH_AGENT_NAME,\n",
    "            \"content\": \"Empty search query is not allowed.\",\n",
    "        }, auto_shift_kv=True)\n",
    "        update_display()\n",
    "        return\n",
    "    \n",
    "    # Collect documents.\n",
    "    documents = pick_relevant_web_documents(\n",
    "        search_query, \n",
    "        embedding=MultilingualE5Small(),\n",
    "        engine=\"duckduckgo\",\n",
    "        n_relevant_chunks=3,\n",
    "        n_search_results=20,\n",
    "    )\n",
    "    \n",
    "    # Format results.\n",
    "    search_result = \"\"\n",
    "    for i, doc in enumerate(documents):\n",
    "        search_result += f\"\"\"# Document Num: {i+1}\n",
    "# Document Title: {doc[\"title\"]}\n",
    "# Document URL: {doc[\"url\"]}\n",
    "# Document Content: {doc[\"content\"]}\n",
    "\n",
    "\"\"\"\n",
    "    search_result = search_result.rstrip()\n",
    "    if len(documents) == 0:\n",
    "        search_result= \"No result hits.\"\n",
    "    referred_urls = list(set([doc[\"url\"] for doc in documents]))\n",
    "\n",
    "    \n",
    "    push_context({\n",
    "        \"role\": SEARCH_AGENT_NAME,\n",
    "        \"content\": \"\",\n",
    "        \"references\": referred_urls,\n",
    "        \"search_result\": search_result,\n",
    "    }, auto_shift_kv=True)\n",
    "    update_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df533939-2566-4e73-ae24-f936d65bf9d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Main loop.\n",
    "@disable_uis\n",
    "def main(sender=None) -> None:\n",
    "    from lib.utils import reformat_python_code\n",
    "\n",
    "    \n",
    "    if session.model == None:\n",
    "        push_context({\n",
    "            \"role\": session.assistant_name, \n",
    "            \"content\": \"Model is not loaded yet. Please select and load a local model before inference.\"\n",
    "        }, auto_shift_kv=False)\n",
    "        update_display()\n",
    "        return\n",
    "        \n",
    "\n",
    "    # Register user's input.\n",
    "    user_message = session.field.value\n",
    "    if session.model.token_count(user_message) >= MAX_USER_TOKENS: return\n",
    "    if user_message == \"\": return\n",
    "    submit_names_action()\n",
    "    \n",
    "    session.field.value = ''\n",
    "    push_context({\n",
    "        \"role\": \"User\", \n",
    "        \"content\": user_message\n",
    "    }, auto_shift_kv=True)\n",
    "    update_display()\n",
    "\n",
    "    \n",
    "    output = predict_stream_with_display()\n",
    "\n",
    "    \n",
    "    # Parse tool.\n",
    "    regex = re.compile(r'```\\n?({tools})(.*)\\n?```'.format(tools='|'.join([tool.name for tool in session.tools])), re.DOTALL)\n",
    "    tool = re.search(regex, output)\n",
    "\n",
    "    \n",
    "    if tool is None:\n",
    "        push_context({\n",
    "            \"role\": session.assistant_name, \n",
    "            \"content\": output,\n",
    "        }, auto_shift_kv=True)\n",
    "        update_display()\n",
    "        \n",
    "    else:\n",
    "        stripped = re.sub(regex, '', output).rstrip()\n",
    "        tool_type, tool_input = tool.group(1), tool.group(2).strip()\n",
    "\n",
    "        if tool_type == \"python\":\n",
    "            reformatted = reformat_python_code(tool_input)\n",
    "            push_context({\n",
    "                \"role\": session.assistant_name,\n",
    "                \"content\": stripped,\n",
    "                \"code\": reformatted,\n",
    "            }, auto_shift_kv=True)\n",
    "            update_display()\n",
    "            python_chain(reformatted)\n",
    "        elif tool_type == \"google\":\n",
    "            push_context({\n",
    "                \"role\": session.assistant_name,\n",
    "                \"content\": stripped,\n",
    "                \"search_query\": tool_input,\n",
    "            }, auto_shift_kv=True)\n",
    "            update_display()\n",
    "            search_chain(tool_input)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Tool {tool_type} is not implemented yet.\")\n",
    "        \n",
    "        # Add reaction to the tool results.\n",
    "        reaction = predict_stream_with_display(additional_stop_tokens=[f\"```{tool.name}\" for tool in session.tools])\n",
    "        push_context({\n",
    "            \"role\": session.assistant_name, \n",
    "            \"content\": reaction\n",
    "        }, auto_shift_kv=True)\n",
    "        update_display()\n",
    "\n",
    "    \n",
    "    export_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547323b-eed7-43dd-973f-812718ecf408",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define UI actions.\n",
    "session.button.on_click(main)\n",
    "session.reset_button.on_click(initialize)\n",
    "session.retrieve.on_click(retrieve_latest_input)\n",
    "session.create_voice.on_click(create_voice_action)\n",
    "session.load_button.on_click(reload_model)\n",
    "session.unload_button.on_click(unload_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aae6c7-1574-4a98-892d-86eadd19773a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def show_guis(show_debug: bool = False) -> None:\n",
    "    html = f\"\"\"<h1>Integrative LLM Chat UI for Jupyter</h1>\n",
    "This is an LLM-powered chat interface integrated with voice synthesis model, web search-based RAG, and python environment.</br>\n",
    "For better results, I strongly recommend you to select a model large enough or trained for tool use.</br>\n",
    "If you wanna use unsupported prompt template, define <code>PromptBuilder</code> class yourself.</br>\n",
    "<br>\n",
    "<code>./{GGUF_DIR}</code>: Directory to put GGUF models.</br>\n",
    "<code>./{AGENT_WORKING_DIR}</code>: Working directory when executing Python code.</br>\n",
    "<code>./{TTS_ASSET_ROOT}</code>: Directory to put TTS models.<br> \n",
    "(The subdirectory name containing the model should match the *.safetensors file name)</br>\n",
    "</br>\n",
    "\"\"\"\n",
    "    HBox = widgets.HBox\n",
    "    generation_params = session.generation_params\n",
    "    display(\n",
    "        HTML(html),\n",
    "        \n",
    "        HTML(\"<h3>Session Options</h3>\"),\n",
    "        HBox([session.template_selector,session.streamingllm]),\n",
    "        HBox([session.user_nickname_field, session.assistant_name_field]),\n",
    "        session.user_preamble,\n",
    "        \n",
    "        HTML(\"<h3>llama-cpp-python GGUF Loader</h3>\"),\n",
    "        HBox([session.llama_cpp_options.define_n_ctx, session.llama_cpp_options.n_gpu_layers, session.llama_cpp_options.flash_attention]),\n",
    "        HBox([session.llama_cpp_options.gguf_selector, session.load_button, session.unload_button]),\n",
    "        session.out,\n",
    "        \n",
    "        HTML(\"<h3>Generation Params</h3>\"),\n",
    "        HBox([generation_params.temperature, generation_params.top_k]),\n",
    "        HBox([generation_params.top_p, generation_params.frequency_penalty]),\n",
    "        HBox([generation_params.presence_penalty, generation_params.repeat_penalty]),\n",
    "        HBox([session.field, session.button, session.guessing_image]),\n",
    "        HBox([session.retrieve, session.reset_button]),\n",
    "        \n",
    "        HTML(\"<h3>Voice Synthesis (last item)</h3>\"),\n",
    "        HBox([session.dropdown, session.voice_length]),\n",
    "        session.create_voice,\n",
    "        session.voice_player,\n",
    "        session.debug if show_debug else HTML(\"\"),\n",
    "    )\n",
    "    \n",
    "ignore_warnings()\n",
    "show_guis(show_debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2-clone",
   "language": "python",
   "name": "llama2-clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
